version: '3.8'

services:
  # Weaviate Vector Store
  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: weaviate
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
    ports:
      - "8080:8080"
    volumes:
      - weaviate_data:/var/lib/weaviate
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/v1/.well-known/ready"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # Ollama for Local LLM
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # Initialize Ollama with mistral model
  ollama-init:
    image: curlimages/curl:latest
    container_name: ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    command: >
      sh -c "
        echo 'Pulling mistral model...';
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"mistral\"}' || true;
        echo 'Model pull initiated';
      "
    restart: "no"

  # FastAPI Application
  app:
    build: .
    container_name: doc_qa_app
    ports:
      - "8000:8000"
    depends_on:
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_healthy
    environment:
      WEAVIATE_URL: http://weaviate:8080
      LLM_BASE_URL: http://ollama:11434
      PYTHONUNBUFFERED: 1
    volumes:
      - ./data:/app/data
      - ./reports:/app/reports
      - ./evaluation:/app/evaluation
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 60s

  # MLflow for Evaluation Tracking
  mlflow:
    image: python:3.11-slim
    container_name: mlflow
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/mlflow
    working_dir: /mlflow
    command: bash -c "pip install mlflow && mlflow server --host 0.0.0.0"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 30s
      timeout: 5s
      retries: 3

volumes:
  weaviate_data:
  ollama_data:
  mlflow_data:
