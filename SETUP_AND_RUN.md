# üöÄ COMPLETE SETUP & RUN GUIDE

## ‚úÖ STATUS: All Tasks Completed
- ‚úÖ Task 1: Document Ingestion & QA System (COMPLETE)
- ‚úÖ Task 2: Evaluation Framework (COMPLETE)
- ‚úÖ Task 3: Query Decomposition & Answer Synthesis (COMPLETE)
- ‚úÖ Task 4: Architecture & Evaluation Report (COMPLETE)

---

## üîë API KEYS NEEDED

**ANSWER: NONE!** ‚úÖ

This system uses **100% local services**:
- ‚úÖ **Ollama** - Local LLM (no API key needed)
- ‚úÖ **Weaviate** - Local vector database (local key only)
- ‚úÖ **Sentence-Transformers** - Local embeddings (no API key)
- ‚úÖ **MLflow** - Local tracking (no API key needed)

The `.env` file already has all necessary local configuration!

---

## üéØ STEP-BY-STEP SETUP

### Step 1: Verify Prerequisites (2 minutes)

Check you have installed:
```powershell
# Check Docker is installed
docker --version

# Check Docker Compose is installed
docker-compose --version
```

If Docker is not installed, download from: https://www.docker.com/products/docker-desktop

### Step 2: Navigate to Project (1 minute)

```powershell
cd "D:\Desktop\Assignment"
```

### Step 3: Start Services (5 minutes)

Start all 4 services (Weaviate, Ollama, API, MLflow):

```powershell
docker-compose up -d
```

**Wait for services to start:**
- Weaviate: ~10 seconds
- Ollama: ~30 seconds  
- API: ~10 seconds
- MLflow: ~10 seconds

Total: ~60 seconds

**Check status:**
```powershell
docker-compose ps
```

You should see 4 containers with status "Up"

### Step 4: Pull LLM Model (5 minutes)

Download the Mistral 7B model to Ollama:

```powershell
docker exec ollama ollama pull mistral
```

‚è±Ô∏è First time: ~3-5 minutes (downloads ~4GB model)
‚úÖ Subsequent times: instant (cached)

### Step 5: Verify System Health (1 minute)

Check that all services are working:

```powershell
# Check API is running
curl http://localhost:8000/health

# Check Weaviate is running
curl http://localhost:8080/v1/.well-known/ready
```

Expected output: `healthy`

### Step 6: Access the System (Ready to use!)

**Interactive API Documentation:**
```
http://localhost:8000/docs
```

**Direct API Testing:**
```powershell
# Ask a question
curl -X POST http://localhost:8000/ask `
  -H "Content-Type: application/json" `
  -d '{"question":"What is machine learning?"}'
```

**MLflow Tracking UI:**
```
http://localhost:5000
```

---

## üìã VERIFICATION CHECKLIST

Run this to verify everything works:

```powershell
python verify_setup.py
```

Expected output:
```
‚úì All imports working
‚úì Config loaded
‚úì Ollama responding
‚úì Weaviate responding
‚úì Embeddings working
‚úì System ready!
```

---

## üéÆ TRY IT NOW

### Option 1: Use Web Interface

1. Open: http://localhost:8000/docs
2. Click the **Try it out** button on `/ask` endpoint
3. Enter: `What is machine learning?`
4. Click **Execute**
5. See the answer!

### Option 2: Use Python

Create `test_qa.py`:

```python
import requests

response = requests.post(
    "http://localhost:8000/ask",
    json={"question": "What is machine learning?"}
)

print(response.json())
```

Run it:
```powershell
python test_qa.py
```

### Option 3: Upload Documents & Ask

```powershell
# Upload a document
curl -X POST http://localhost:8000/upload `
  -F "file=@data/machine_learning_guide.txt"

# Ask a question
curl -X POST http://localhost:8000/ask `
  -H "Content-Type: application/json" `
  -d '{"question":"What are neural networks?"}'
```

---

## üîß CONFIGURATION

### What's in `.env`?

| Setting | Value | Purpose |
|---------|-------|---------|
| `WEAVIATE_URL` | `http://localhost:8080` | Vector database location |
| `WEAVIATE_API_KEY` | `weaviate_key` | Local database password |
| `LLM_MODEL` | `mistral` | Which model to use |
| `LLM_BASE_URL` | `http://localhost:11434` | Ollama server |
| `EMBEDDING_MODEL` | `all-MiniLM-L6-v2` | Embedding model |
| `API_PORT` | `8000` | API server port |
| `API_HOST` | `0.0.0.0` | API server host |

**No changes needed!** All values are pre-configured for local development.

### Port Reference

| Service | Port | URL |
|---------|------|-----|
| API Server | 8000 | http://localhost:8000 |
| Weaviate | 8080 | http://localhost:8080 |
| Ollama | 11434 | http://localhost:11434 |
| MLflow | 5000 | http://localhost:5000 |

---

## üß™ RUN EVALUATION

Test the system with sample data:

```powershell
python run_evaluation.py
```

This will:
- Load sample documents
- Run 5 test cases
- Calculate RAGAS metrics
- Save results to `reports/evaluation_results.json`

---

## üõë STOP THE SYSTEM

When done:

```powershell
docker-compose down
```

This stops all services but preserves data.

**Remove everything:**
```powershell
docker-compose down -v
```

This removes containers and volumes (fresh start next time).

---

## üêõ TROUBLESHOOTING

### Problem: "Docker not running"
```powershell
# Start Docker Desktop (or Docker daemon)
# Then try again:
docker-compose up -d
```

### Problem: "Port 8000 already in use"
```powershell
# Find what's using port 8000
netstat -ano | findstr :8000

# Kill the process
taskkill /PID <PID> /F

# Or change API_PORT in .env and restart
```

### Problem: "curl not found"
```powershell
# Use PowerShell instead:
Invoke-WebRequest http://localhost:8000/health

# Or use Python:
python -c "import requests; print(requests.get('http://localhost:8000/health').json())"
```

### Problem: "Ollama model download fails"
```powershell
# Check Ollama is running
docker logs ollama

# Try again
docker exec ollama ollama pull mistral

# Or use a smaller model
docker exec ollama ollama pull neural-chat
```

### Problem: "Services not starting"
```powershell
# Check logs
docker-compose logs

# Rebuild containers
docker-compose up -d --build

# Start from scratch
docker-compose down -v
docker-compose up -d
```

---

## üìä WHAT YOU'RE GETTING

### Available Endpoints (16 total)

**Document Management:**
- `POST /upload` - Upload single document
- `POST /upload-batch` - Upload multiple documents
- `GET /documents` - List loaded documents

**Question Answering:**
- `POST /ask` - Ask a question about documents
- `POST /ask-follow-up` - Ask follow-up question

**Conversation:**
- `GET /conversation-history` - Get conversation history
- `POST /clear-conversation` - Clear history

**System:**
- `GET /health` - Check system health
- `GET /system-info` - Get system information
- `GET /config` - View configuration

**Advanced:**
- `POST /decompose-query` - Decompose complex query
- `POST /retrieve-context` - Retrieve relevant context
- `POST /evaluate` - Evaluate with test cases
- `POST /batch-evaluate` - Batch evaluation
- `GET /evaluation-results` - Get evaluation results

---

## ‚ú® SAMPLE WORKFLOW

```powershell
# 1. Start system
docker-compose up -d

# 2. Wait 60 seconds
Start-Sleep -Seconds 60

# 3. Pull model
docker exec ollama ollama pull mistral

# 4. Verify health
curl http://localhost:8000/health

# 5. Upload sample documents
curl -X POST http://localhost:8000/upload `
  -F "file=@data/machine_learning_guide.txt"

curl -X POST http://localhost:8000/upload `
  -F "file=@data/data_science_fundamentals.txt"

# 6. Ask questions
curl -X POST http://localhost:8000/ask `
  -H "Content-Type: application/json" `
  -d '{"question":"What is supervised learning?"}'

# 7. Open web UI
Start-Process http://localhost:8000/docs

# 8. Run evaluation
python run_evaluation.py

# 9. View results
Get-Content reports/evaluation_results.json | ConvertFrom-Json | Format-Table

# 10. Stop when done
docker-compose down
```

---

## üìö NEXT STEPS

1. **Run the setup:** Follow steps 1-6 above
2. **Test the API:** Visit http://localhost:8000/docs
3. **Upload documents:** Use `/upload` endpoint
4. **Ask questions:** Use `/ask` endpoint
5. **Evaluate:** Run `python run_evaluation.py`
6. **View results:** Check `reports/` directory

---

## üéì LEARN MORE

- **Quick Start:** Read [START_HERE.md](START_HERE.md)
- **Architecture:** Read [ARCHITECTURE_AND_EVALUATION_REPORT.md](ARCHITECTURE_AND_EVALUATION_REPORT.md)
- **API Details:** Read [README.md](README.md)
- **Usage Guide:** Read [USAGE_GUIDE.md](USAGE_GUIDE.md)

---

## ‚úÖ READY?

All 4 tasks are complete. The system is production-ready.

**Next action:** Run `docker-compose up -d` and start using the system!

---

Last Updated: January 19, 2026
Status: ‚úÖ READY FOR PRODUCTION
